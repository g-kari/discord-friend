# Ollama å®Ÿè·µæ´»ç”¨ã‚¬ã‚¤ãƒ‰

Ollamaã‚’ä½¿ã£ãŸãƒ­ãƒ¼ã‚«ãƒ«LLMç’°å¢ƒã®æ§‹ç¯‰ã¨æœ€é©åŒ–ã®ãŸã‚ã®å®Ÿè·µçš„ãªãƒ’ãƒ³ãƒˆé›†ã§ã™ã€‚

> **ğŸ“š é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: 
> - **[README.md](README.md)** - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦ãƒ»ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ–¹æ³•
> - **[CLAUDE.md](CLAUDE.md)** - æŠ€è¡“è©³ç´°ãƒ»é–‹ç™ºã‚¬ã‚¤ãƒ‰
> - **[scripts/README.md](scripts/README.md)** - é–‹ç™ºæ”¯æ´ãƒ„ãƒ¼ãƒ«

---

## ğŸš€ Ollamaã®åˆ©ç‚¹

### ãªãœOllamaãŒå„ªã‚Œã¦ã„ã‚‹ã®ã‹
- **ç°¡å˜ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**: ãƒ¯ãƒ³ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»èµ·å‹•
- **ãƒ¢ãƒ‡ãƒ«ç®¡ç†**: `ollama pull/push/list` ã§Dockerãƒ©ã‚¤ã‚¯ãªç®¡ç†
- **è»½é‡**: CPUã§ã‚‚å‹•ä½œã€GPUãŒã‚ã‚Œã°é«˜é€ŸåŒ–
- **APIäº’æ›æ€§**: OpenAI APIäº’æ›ã§æ—¢å­˜ãƒ„ãƒ¼ãƒ«ã¨é€£æºã—ã‚„ã™ã„
- **æ—¥æœ¬èªå¯¾å¿œ**: Qwen2.5ãªã©å„ªç§€ãªæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½

## ğŸ“¥ ãŠã™ã™ã‚æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«

### 1. Qwen2.5ã‚·ãƒªãƒ¼ã‚ºï¼ˆæœ€æ¨å¥¨ï¼‰
```bash
# è»½é‡ãƒ»é«˜é€Ÿç‰ˆï¼ˆ7Bï¼‰
ollama pull qwen2.5:7b

# é«˜å“è³ªç‰ˆï¼ˆ14Bï¼‰ - ãƒ¡ãƒ¢ãƒª16GBä»¥ä¸Šæ¨å¥¨
ollama pull qwen2.5:14b

# è¶…è»½é‡ç‰ˆï¼ˆ3Bï¼‰ - ä½ã‚¹ãƒšãƒƒã‚¯ç’°å¢ƒå‘ã‘
ollama pull qwen2.5:3b
```

### 2. ãã®ä»–ã®æ—¥æœ¬èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«
```bash
# Google Gemma 2ï¼ˆæ—¥æœ¬èªã‚‚å¯¾å¿œï¼‰
ollama pull gemma2:9b-instruct-q4_K_M

# Llama 3.1 æ—¥æœ¬èªç‰ˆ
ollama pull llama3.1:8b

# å®Ÿé¨“çš„ï¼šæ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«
ollama pull elyza:8b
```

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
```bash
# ~/.bashrc ã¾ãŸã¯ ~/.zshrc ã«è¿½åŠ 

# åŒæ™‚å‡¦ç†æ•°ã®èª¿æ•´ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰
export OLLAMA_NUM_PARALLEL=4

# æœ€å¤§ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ‡ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3ï¼‰
export OLLAMA_MAX_LOADED_MODELS=2

# CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: è‡ªå‹•ï¼‰
export OLLAMA_NUM_THREAD=8

# ãƒ›ã‚¹ãƒˆã‚¢ãƒ‰ãƒ¬ã‚¹ï¼ˆå¤–éƒ¨ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ï¼‰
export OLLAMA_HOST=0.0.0.0

# ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆã®å¤‰æ›´ï¼ˆå®¹é‡ãŒå¤§ãã„å ´åˆï¼‰
export OLLAMA_MODELS=/path/to/large/storage/ollama/models
```

### 2. GPUæœ€é©åŒ–ï¼ˆNVIDIA GPUä½¿ç”¨æ™‚ï¼‰
```bash
# GPUä½¿ç”¨çŠ¶æ³ã®ç¢ºèª
nvidia-smi

# CUDAå¯¾å¿œã®ç¢ºèª
ollama run qwen2.5:7b --verbose

# GPUä½¿ç”¨ç‡ã®èª¿æ•´
export CUDA_VISIBLE_DEVICES=0  # ç‰¹å®šã®GPUã‚’ä½¿ç”¨
```

### 3. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
```bash
# ãƒ¢ãƒ‡ãƒ«é‡å­åŒ–ç‰ˆã®ä½¿ç”¨ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰
ollama pull qwen2.5:7b-q4_K_M  # 4bité‡å­åŒ–ç‰ˆ
ollama pull qwen2.5:7b-q5_K_M  # 5bité‡å­åŒ–ç‰ˆ
ollama pull qwen2.5:7b-q8_0    # 8bité‡å­åŒ–ç‰ˆ
```

## ğŸ› ï¸ å®Ÿç”¨çš„ãªä½¿ã„æ–¹

### 1. ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
```bash
# Modelfileã‚’ä½œæˆ
cat > CustomJapaneseAssistant <<EOF
FROM qwen2.5:7b

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
SYSTEM """
ã‚ãªãŸã¯è¦ªåˆ‡ã§ä¸å¯§ãªæ—¥æœ¬èªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€åˆ†ã‹ã‚Šã‚„ã™ãé©åˆ‡ãªæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
æŠ€è¡“çš„ãªå†…å®¹ã‚‚ã€åˆå¿ƒè€…ã«ã‚‚ç†è§£ã§ãã‚‹ã‚ˆã†ã«èª¬æ˜ã™ã‚‹ã“ã¨ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
"""

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_predict 2048
PARAMETER repeat_penalty 1.1
EOF

# ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
ollama create japanese-assistant -f CustomJapaneseAssistant

# ä½¿ç”¨
ollama run japanese-assistant
```

### 2. APIçµŒç”±ã§ã®åˆ©ç”¨
```python
# Pythonä¾‹
import requests
import json

def query_ollama(prompt, model="qwen2.5:7b"):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    
    response = requests.post(url, json=payload)
    return response.json()['response']

# ä½¿ç”¨ä¾‹
result = query_ollama("Pythonã®åŸºç¤ã‚’ç°¡å˜ã«èª¬æ˜ã—ã¦ãã ã•ã„")
print(result)
```

### 3. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹
```python
import requests
import json

def stream_ollama(prompt, model="qwen2.5:7b"):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": True
    }
    
    with requests.post(url, json=payload, stream=True) as response:
        for line in response.iter_lines():
            if line:
                chunk = json.loads(line)
                if 'response' in chunk:
                    print(chunk['response'], end='', flush=True)

# ä½¿ç”¨ä¾‹
stream_ollama("æ—¥æœ¬ã®å››å­£ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦ãã ã•ã„")
```

## ğŸ”§ ä¾¿åˆ©ãªã‚³ãƒãƒ³ãƒ‰é›†

### ãƒ¢ãƒ‡ãƒ«ç®¡ç†
```bash
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
ollama list

# ãƒ¢ãƒ‡ãƒ«è©³ç´°æƒ…å ±
ollama show qwen2.5:7b

# ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ”ãƒ¼ï¼ˆåˆ¥åä½œæˆï¼‰
ollama cp qwen2.5:7b my-japanese-model

# ä¸è¦ãªãƒ¢ãƒ‡ãƒ«ã®å‰Šé™¤
ollama rm old-model

# ãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°
ollama pull qwen2.5:7b
```

### ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
```bash
# ãƒ­ã‚°ã®ç¢ºèª
journalctl -u ollama -f  # systemdã®å ´åˆ

# ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª
systemctl status ollama

# ãƒãƒ¼ãƒˆä½¿ç”¨ç¢ºèª
lsof -i :11434

# ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€
ls ~/.ollama/models/

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
rm -rf ~/.ollama/models/blobs/
```

## ğŸ¯ Discord Botã¨ã®çµ±åˆæœ€é©åŒ–

> **ğŸ’¡ çµ±åˆã‚¬ã‚¤ãƒ‰**: Discord Bot ã§ã® Ollama ä½¿ç”¨æ–¹æ³•ã¯ [README.md](README.md) ã¨ [CLAUDE.md](CLAUDE.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

### 1. æ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®š
```python
# Ollamaç”¨ã®æ¥ç¶šãƒ—ãƒ¼ãƒ«
import aiohttp
from aiohttp import TCPConnector

# æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®è¨­å®š
connector = TCPConnector(
    limit=100,              # ç·æ¥ç¶šæ•°åˆ¶é™
    limit_per_host=30,      # ãƒ›ã‚¹ãƒˆã”ã¨ã®æ¥ç¶šæ•°åˆ¶é™
    ttl_dns_cache=300,      # DNSã‚­ãƒ£ãƒƒã‚·ãƒ¥æ™‚é–“
    enable_cleanup_closed=True
)

session = aiohttp.ClientSession(connector=connector)
```

### 2. ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æœ€é©åŒ–
```python
# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
timeout = aiohttp.ClientTimeout(
    total=30,      # ç·ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    connect=5,     # æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
    sock_read=25   # èª­ã¿å–ã‚Šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
)

# çŸ­ã„å¿œç­”ç”¨ã®è¨­å®š
quick_response_params = {
    "temperature": 0.5,
    "num_predict": 256,
    "top_k": 40,
    "repeat_penalty": 1.2
}
```

### 3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
```python
async def safe_ollama_query(prompt, retries=3):
    """å®‰å…¨ãªOllamaã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
    for attempt in range(retries):
        try:
            response = await query_ollama(prompt)
            return response
        except aiohttp.ClientError as e:
            if attempt == retries - 1:
                raise
            await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
```

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### 1. ç°¡æ˜“ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```bash
#!/bin/bash
# ollama-monitor.sh

while true; do
    clear
    echo "=== Ollama Status ==="
    echo "Time: $(date)"
    echo ""
    
    # ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹
    echo "Process:"
    ps aux | grep ollama | grep -v grep
    echo ""
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
    echo "Memory Usage:"
    ps -o pid,vsz,rss,comm -p $(pgrep ollama) 2>/dev/null
    echo ""
    
    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªæ¥ç¶š
    echo "Active Connections:"
    lsof -i :11434 | tail -n +2 | wc -l
    echo ""
    
    # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§
    echo "Loaded Models:"
    ollama list | tail -n +2
    
    sleep 5
done
```

### 2. Prometheus ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
```python
# Ollamaã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆã‚‚ã—å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
import aiohttp
import asyncio

async def get_ollama_metrics():
    async with aiohttp.ClientSession() as session:
        async with session.get('http://localhost:11434/metrics') as resp:
            return await resp.text()
```

## ğŸ³ Docker Composeçµ±åˆ

### docker-compose.yml
```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=2
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 8G
    restart: unless-stopped
    # GPUã‚µãƒãƒ¼ãƒˆï¼ˆNVIDIAï¼‰
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚³ãƒ³ãƒ†ãƒŠ
  ollama-setup:
    image: ollama/ollama:latest
    depends_on:
      - ollama
    volumes:
      - ollama-data:/root/.ollama
    entrypoint: |
      sh -c "
        sleep 10
        ollama pull qwen2.5:7b
        ollama pull qwen2.5:14b
        echo 'Models loaded successfully'
      "

volumes:
  ollama-data:
```

## ğŸ‰ ã¾ã¨ã‚

Ollamaã¯ä»¥ä¸‹ã®ç‚¹ã§å„ªã‚Œã¦ã„ã¾ã™ï¼š

1. **ç°¡å˜**: æ•°ã‚³ãƒãƒ³ãƒ‰ã§é«˜å“è³ªãªLLMãŒä½¿ãˆã‚‹
2. **è»½é‡**: ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ãŒè‰¯ãã€ä½ã‚¹ãƒšãƒƒã‚¯ã§ã‚‚å‹•ä½œ
3. **æŸ”è»Ÿ**: ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ã§æ§˜ã€…ãªç”¨é€”ã«å¯¾å¿œ
4. **çµ±åˆã—ã‚„ã™ã„**: æ¨™æº–çš„ãªAPIã§æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨é€£æº

ç‰¹ã«Qwen2.5ã‚·ãƒªãƒ¼ã‚ºã¨ã®çµ„ã¿åˆã‚ã›ã¯ã€æ—¥æœ¬èªå‡¦ç†ã«ãŠã„ã¦éå¸¸ã«å„ªã‚ŒãŸçµæœã‚’æä¾›ã—ã¾ã™ã€‚

Happy Ollama-ing! ğŸ¦™